#include "./components/action.hpp"
#include "./components/belief.hpp"
#include "./components/history.hpp"
#include "./components/state.hpp"

Action action_prog_widen(History h){
    if(C_h.size() <= k_a * std::pow(N(h), alpha_a)){
        Action a = next_action(h);
        C_h.append(a);
    }
    Action best_a;
    double best_reward = -INFINITY;
    for(Action a : C_h){
        double reward = Q(h.append(a)) + c * std::sqrt(std::log(N(h))/ N(h.append(a)));

        // N: a count of the number of visits.
        // M: a count of the number of times that a history has been generated by the model.
        if (reward > best_reward){
            best_a = a;
        }
    }
    return best_a;
}

int d_max = 5;

Action Plan(ParticleFilterBelief b){
    for(int i = 0; i < n; i++){
        /* 
           the belief is a distribution of states.
           we can sample a state from the belief.
        */
        State s = ParticleFilterBelief::sample_from_belief(b);
 
        /* simulate the POMCP, with state s, 
         and belief, and a max depth
         of monte carlo sample tree (MCST) */
        simulate_pomcp_dpw(s, b, d_max); 
    }
}

int main(){

}